\section{Background}\label{sec:background}
\subsection{Neural Machine Translation}
A recent development in deep learning is sequence-to-sequence learning for Neural Machine Translation \cite{sutskever2014sequence, cho2014learning}. Translation can be seen as a probabilistic process, where the goal is to find a target sentence $\mathbf{y} = (y_1,...,y_n)$ from a source sequence $\mathbf{x} = (x_1,...,x_m)$ that maximizes the conditional probability of $\mathbf{y}$ given the source sentence $\mathbf{x}$, mathematically depicted as $\argmax_y p(y|x)$ \cite{bahdanau2014neural}. But this conditional distribution is of course not given and has to be learned by a model from the supplied data.  
\citeauthor{sutskever2014sequence} \cite{sutskever2014sequence} and \citeauthor{cho2014learning} \cite{cho2014learning} both have proposed a structure to learn this distribution: a model that consists of a encoder and decoder component that are trained simultaneously. The encoder component tries to encode the variable length input sequence $\mathbf{x}$ to a fixed length hidden state vector. This hidden state is then supplied to the decoder component that tries to decode it into the variable length target sequence $\mathbf{y}$. A Recurrent Neural Network (RNN) is used in this to sequentially read the variable length sequence and produces a fixed size vector.

Over the years, different architectural changes of encoder and decoder components were proposed. \citeauthor{sutskever2014sequence} \cite{sutskever2014sequence} introduces multi-layered RNN with Long-Short-Term memory units (LSTM), where both the encoder and decoder consist of multiple layers (4 in their research). Each layer in the encoder produces a fixed size hidden state that is passed onto the corresponding layer in the decoder, where the results are combined into a target sequence prediction. One unexplainable factor noted by the authors of this architecture is that it produces better results if the source sequence is reversed. Note that in each step during decoding, the LSTM only has access to the hidden state from the previous timestep, and the previous predicted token.

\citeauthor{cho2014learning} \cite{cho2014learning} uses a slightly different approach in their model and uses Gated Recurrent Units (GRU) as RNN components. The encoder reads the source sequence sequentially and produces a hidden state, denoted as the context vector. Decoding of a token can now be done based on the previously hidden state of the decoder, the previous predicted token, and the generated context vector. The intuition of this architecture is that it reduces information compression as each decoder step has access to the whole source sequence. The decoder hidden states now only need to retrain information that was previously predicted.

Still, the performance of this process suffers when input sentences start to increase and the information can not be compressed into hidden states \cite{cho2014learning}. \citeauthor{bahdanau2014neural} \cite{bahdanau2014neural} therefore extended the encoder decoder model such that it learns to align and translate jointly with the help of attention. At each time decoding step, the model searches for a set of position in the source sentence where the most relevant information is concentrated. The context vectors corresponding to these positions and the previous generated predicted tokens are then used for prediction of the target sequence. It is also possible to compute attention in different ways as shown by \citeauthor{luong2015effective} \cite{luong2015effective}.

\subsection{Evaluation Metrics}
BLEU \citep{papineni_bleu:_2001} is the most frequently used similarity metric to evaluate the quality of machine translations. BLEU measures how many word sequences from the reference text occur in the generated text and uses a (slightly modified) \textit{n-gram precision} to generate a score. Sentences with the most overlapping n-grams score highest. BLEU can be used to calculate the quality of an entire set of <reference,generated> text pairs, which enables researchers to accurately compare the performance of different models on the same dataset. BLEU can be configured with different n-gram sizes, which is denoted by BLEU-\textit{n} (e.g. BLEU-4).

Another widely used metric is ROUGE \citep{lin_rouge:_2004}. ROUGE can be used to calculate recall and F1 scores in addition to precision. This is done by looking at which n-grams in the generated text occur in the reference text. ROUGE is often used to evaluate the quality of machine-generated text summaries, where a word-for-word reproduction of the reference text that gives a high BLEU score is not appreciated. Still, the generated summary should reflect the original text. ROUGE has a number of specialized extensions, of which ROUGE-L is most appropriate to evaluate commit messages. ROUGE-L measures the longest common subsequence between messages to ``capture the sentence level structure in a natural way'' \citep{lin_rouge:_2004}.

Lastly, METEOR is a similarity metric that uses the harmonic mean between precision and recall. METEOR attempts to correct a number of issues with BLEU, such as the fact that sentences have to be identical to get the highest score and that a higher BLEU score not always equals a better translation. The metric is computed using ``a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference'' \citep{banerjee_meteor:_2005}.


\section{Related Work}\label{sec:relatedwork}

The first works about commit message generation were published independently at the same time by \citet{loyola_neural_2017} and \citet{jiang_automatically_2017}. Both approaches feature a similar attentional RNN encoder-decoder architecture.

\citet{loyola_neural_2017} use a vanilla encoder-decoder architecture, similar to the architecture \citet{iyer_summarizing_2016} used for code summarization. The encoder network is simply a lookup table for the input token embedding. The decoder network is a RNN with dropout-regularized \textit{long short-term memory} (LSTM) cells. Dropout is also used at the encoder layer and reduces the risk of overfitting on the training data. A global attention model is used to help the decoder focus on the most important parts of the diffs.

\citet{jiang_automatically_2017} propose a more intricate architecture, where the encoder network is also a RNN. This way, the token embedding can be trained for better model performance. The authors do not implement the network themselves, but instead use \textsc{Nematus}, a specialized toolkit for neural machine translation \cite{sennrich_nematus:_2017}. Besides using dropout in all layers, Nematus also uses the computationally more efficient GRU cells instead of LSTM cells.

\citet{liu_neural-machine-translation-based_2018} investigate the model and results of \cite{jiang_automatically_2017} and found that memorization is the largest contributor to their good results. For almost all correctly generated commit messages, a very similar commits was found in the training set. By removing the noisy commits, the model performance drops by 55\%. To illustrate the shortcomings, \citet{liu_neural-machine-translation-based_2018} propose \textit{NNGen}, a naive nearest-neighbor based approach that re-uses commit messages from similar diffs. \textit{NNGen} outperforms \cite{jiang_automatically_2017} by 20\% in terms of BLEU score, which underlines the similarity in the training and test sets.

The most recent work on commit message generation, by \citet{liu_generating_2019}, states that the main drawback of the earlier approaches by \citet{loyola_neural_2017} and \citet{jiang_automatically_2017} is the inability to generate out-of-vocabulary (OOV) words. Commit messages often contain references to specific class names or functions, related to unique code from a project. When this identifier is omitted from a predicted commit message, it might not make sense.

To mitigate this problem, pointer-generator network \textsc{PtrGNCMsg} is introduced: an improved sequence-to-sequence model that is able to copy words from the input sequence with a certain probability \cite{liu_generating_2019}. The network uses an adapted attentional RNN encoder-decoder architecture, where, at each decoder prediction step, ``the RNN decoder calculates the probability $p_\text{copy}$ of copying words from the source sentence according to the attention distribution, and the probability $p_\text{vocab}$ of selecting words in the vocabulary'' \cite{liu_generating_2019}. By combining the vocabulary, input sequence and probabilities, the model is able to generate valid commit messages containing OOV words.
